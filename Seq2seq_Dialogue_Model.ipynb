{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Seq2seq_Dialogue_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pulSn6bHUOTV"
      },
      "source": [
        "# Creating an End-To-End Dialogue System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_Z5dCsUQnW"
      },
      "source": [
        "In this project we want to create end-to-end dialogue systems, following on from the seq2seq MT labs you've done. Customer support apps and online helpdesks are among the places where conversational models can be used. Retrieval-based models, which produce predefined responses to questions of specific types, are often used to power these models. In this seq2seq model is used to build a generative model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3q5lgWkVD6-"
      },
      "source": [
        "To begin, download the data ZIP file from [here](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and place it under the current directory. \n",
        "\n",
        "After that, let’s import some necessities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dz-W5nwGydd"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
        "np.random.seed(1)\n",
        "random.seed(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVIb8_AK9xjD"
      },
      "source": [
        "# The Cornell Movie-Dialogs Corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7hru-2uFRWy"
      },
      "source": [
        "The next move is to reformat our data file and load the data into functional structures. \n",
        "\n",
        "The Cornell Movie-Dialogs Corpus contains 220,579 conversational exchanges between 10,292 pairs of movie characters, 9,035 characters from 617 movies, and 304,713 total utterances. This dataset is large with a wide variety of language formality, time periods, and other variables. Our hope is that this variety will make our model responsive to a wide range of queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVCjdDP_HMIM",
        "outputId": "7e66beff-93b1-477c-c189-3cf890dafe63"
      },
      "source": [
        "#Loading the data\n",
        "lines = open('/content/cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('/content/cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n') # index of related lines\n",
        "\n",
        "# Create a dictionary to map each id with its line\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        "        \n",
        "# Create a list of all of the ids.\n",
        "convs = [ ]\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))\n",
        "\n",
        "# Sort the sentences into questions (inputs) and answers (targets)\n",
        "pairs = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        pairs.append([id2line[conv[i]],id2line[conv[i+1]]])\n",
        "        \n",
        "limit = 0\n",
        "for i in range(limit, limit+5):\n",
        "    print(pairs[i][0])\n",
        "    print(pairs[i][1])\n",
        "    print()\n",
        "    \n",
        "len(pairs)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "You're asking me out.  That's so cute. What's your name again?\n",
            "Forget it.\n",
            "\n",
            "No, no, it's my fault -- we didn't have a proper introduction ---\n",
            "Cameron.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221616"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhwqopLhZBu"
      },
      "source": [
        "We'll format data file with a question sentence and an answer sentence pair on each line for convenience.  Before we are ready to use this data, we must perform some preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0O-qSHrHYgU",
        "outputId": "b3c3c6f6-ff95-40f5-8a59-fec14c3e68c3"
      },
      "source": [
        "def preprocess(pairs):\n",
        "    p = pairs.copy()\n",
        "\n",
        "    for i in p:\n",
        "        for j in range(0,2):\n",
        "            i[j] = i[j].lower()\n",
        "            i[j] = re.sub(r\"there's\", \"there is\", i[j])\n",
        "            i[j] = re.sub(r\"i'm\", \"i am\", i[j])\n",
        "            i[j] = re.sub(r\"he's\", \"he is\", i[j])\n",
        "            i[j] = re.sub(r\"she's\", \"she is\", i[j])\n",
        "            i[j] = re.sub(r\"it's\", \"it is\", i[j])\n",
        "            i[j] = re.sub(r\"that's\", \"that is\", i[j])\n",
        "            i[j] = re.sub(r\"what's\", \"that is\", i[j])\n",
        "            i[j] = re.sub(r\"where's\", \"where is\", i[j])\n",
        "            i[j] = re.sub(r\"how's\", \"how is\", i[j])\n",
        "            i[j] = re.sub(r\"\\'ll\", \" will\", i[j])\n",
        "            i[j] = re.sub(r\"\\'ve\", \" have\", i[j])\n",
        "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
        "            i[j] = re.sub(r\"\\'d\", \" would\", i[j])\n",
        "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
        "            i[j] = re.sub(r\"won't\", \"will not\", i[j])\n",
        "            i[j] = re.sub(r\"can't\", \"cannot\", i[j])\n",
        "            i[j] = re.sub(r\"n't\", \" not\", i[j])\n",
        "            i[j] = re.sub(r\"n'\", \"ng\", i[j])\n",
        "            i[j] = re.sub(r\"'bout\", \"about\", i[j])\n",
        "            i[j] = re.sub(r\"'til\", \"until\", i[j])\n",
        "            i[j] = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", i[j])\n",
        "            i[j] = i[j].strip()\n",
        "    return p\n",
        "\n",
        "replaced_pairs = preprocess(pairs)\n",
        "def clean_data(pairs):\n",
        "    p = pairs.copy()\n",
        "    # prepare translation table \n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for i in p:\n",
        "        # tokenize\n",
        "        i[0], i[1] = i[0].split(), i[1].split()\n",
        "        # convert to lower case\n",
        "        i[0], i[1] = [word.lower() for word in i[0]], [word.lower() for word in i[1]]\n",
        "        # remove punctuations \n",
        "        i[0], i[1] = [w.translate(table) for w in i[0]], [w.translate(table) for w in i[1]]\n",
        "        # remove numbers \n",
        "        i[0], i[1] = [word for word in i[0] if word.isalpha()], [word for word in i[1] if word.isalpha()]\n",
        "        # store as string\n",
        "        i[0], i[1] =  ' '.join(i[0]), ' '.join(i[1])\n",
        "            \n",
        "    return p\n",
        "\n",
        "clean_pairs = clean_data(replaced_pairs)\n",
        "clean_pairs[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again',\n",
              "  'well i thought we would start with pronunciation if that is okay with you'],\n",
              " ['well i thought we would start with pronunciation if that is okay with you',\n",
              "  'not the hacking and gagging and spitting part please'],\n",
              " ['not the hacking and gagging and spitting part please',\n",
              "  'okay then how about we try out some french cuisine saturday night'],\n",
              " ['you are asking me out that is so cute that is your name again',\n",
              "  'forget it'],\n",
              " ['no no it is my fault we did not have a proper introduction', 'cameron']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKTpzSOWnUgL"
      },
      "source": [
        "\n",
        "The parsing of the raw movie lines.txt data file is made simpler with the following functions. We add the start and end tokens to our sentences. We also need to find the maximum length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_P2abDKHkbm",
        "outputId": "c6710864-b524-4f94-dedc-7bb054f8505c"
      },
      "source": [
        "# adding the start and end tokens to our utterances\n",
        "start_token = '<startseq>'\n",
        "end_token = '<endseq>'\n",
        "\n",
        "def add_end_start_tokens(pairs):\n",
        "    p = pairs.copy()\n",
        "    for i in p:\n",
        "        i[0] = start_token + ' '  + i[0] + ' ' + end_token\n",
        "        i[1] = start_token + ' '  + i[1] + ' ' + end_token\n",
        "    return p\n",
        "\n",
        "tokenized_pairs = add_end_start_tokens(clean_pairs)\n",
        "tokenized_pairs[:5]\n",
        "\n",
        "# finding the maximum length for questions and answers\n",
        "# we caculate the max length that covers 80% of the data \n",
        "def max_length(pairs,prct):\n",
        "    # Create a list of all the utterances\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for i in pairs:\n",
        "        questions.append(i[0])\n",
        "        answers.append(i[1])\n",
        "        \n",
        "    length_questions = list(len(d.split()) for d in questions)\n",
        "    length_answers = list(len(d.split()) for d in answers)\n",
        "\n",
        "    return int(np.percentile(length_questions, prct)),int(np.percentile(length_answers, prct))\n",
        "\n",
        "max_len_q,max_len_a = max_length(tokenized_pairs,80)\n",
        "\n",
        "print('max-len of questions for training: ', max_len_q)\n",
        "print('max-len of answers for training: ', max_len_a)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max-len of questions for training:  18\n",
            "max-len of answers for training:  18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFsKDr1Wr6d3"
      },
      "source": [
        "The next step is to build a vocabulary and save query/response pairs. \n",
        "It's worth noting that we're dealing with word sequences that don't have an implicit mapping to a discrete numerical space. As a result, we must build one by assigning an index value to each unique word in our dataset.\n",
        "\n",
        "We are going to create our vocabulary. Trimming rarely used words from our vocabulary is another strategy for achieving faster convergence during preparation. The complexity of the function that the model must learn to approximate will be lowered as the feature space is reduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y20Cn9KPHrid",
        "outputId": "38b0f4a7-1d17-41c0-bd17-bb2d23ef1898"
      },
      "source": [
        "# Remove questions and answers that are shorter than 2 words and longer than maxlen.\n",
        "min_line_len = 2 # two words are for tokens\n",
        "\n",
        "def set_length(tokenized_pairs):\n",
        "    pairs_final = []\n",
        "    for p in tokenized_pairs:\n",
        "        if (\n",
        "            len(p[0].split())>=min_line_len and len(p[1].split())>=min_line_len \n",
        "           and len(p[0].split())<=max_len_q and len(p[1].split())<=max_len_a):\n",
        "                \n",
        "            pairs_final.append(p)\n",
        "            \n",
        "    return pairs_final\n",
        "\n",
        "pairs_final = set_length(tokenized_pairs)\n",
        "len(pairs_final)\n",
        "\n",
        "# making a vocabulary of the words that occur more than word_count_threshold \n",
        "def create_reoccurring_vocab(pairs, word_count_threshold = 5):\n",
        "    p = pairs\n",
        "    all_captions = []\n",
        "    for i in p:\n",
        "        for j in i:\n",
        "            all_captions.append(j)\n",
        "\n",
        "    # Consider only words which occur at least 10 times in the corpus\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in all_captions:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "    vocab = list(set(vocab))\n",
        "    print('Short vocab size: %d ' % len(vocab))\n",
        "    return vocab\n",
        "\n",
        "# each word in the vocabulary must be used in the data at least 20 times\n",
        "new_vocab = create_reoccurring_vocab(pairs_final, word_count_threshold = 4)\n",
        "for v in new_vocab:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        new_vocab.remove(v) \n",
        "\n",
        "new_vocab = sorted(new_vocab)[1:]\n",
        "new_vocab[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short vocab size: 14524 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<endseq>', '<startseq>', 'a', 'aa', 'aaaah']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gf7WtdIH1l7",
        "outputId": "39b0895b-70de-4827-8816-8361dd3fbd2f"
      },
      "source": [
        "vocab_len = len(new_vocab) + 1 # since index 0 is used as padding, we have to increase the vocab size\n",
        "vocab_len\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14500"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ1ai5lcibcU"
      },
      "source": [
        "We are going to create a dataset of pairs without the trimmed words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0iu6Z3RmecS"
      },
      "source": [
        "def progressBar(value, endvalue, bar_length=20, job=''):\n",
        "\n",
        "    percent = float(value) / endvalue\n",
        "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
        "    spaces = ' ' * (bar_length - len(arrow))\n",
        "\n",
        "    sys.stdout.write(\"\\r{0} : [{1}] {2}%\".format(job,arrow + spaces, int(round(percent * 100))))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "def print_tensor(t):\n",
        "    print(K.get_value(t))\n",
        "    \n",
        "def to_tensor(t):\n",
        "    return tf.convert_to_tensor(t)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xWI1J7uH4II",
        "outputId": "9ad27b89-4a19-4549-a59f-b7297dd0d2f2"
      },
      "source": [
        "# keeping the pairs with words in the vocab\n",
        "def trimRareWords(voc, pairs):\n",
        "    # Filtering out the pairs with the oov words\n",
        "    keep_pairs = []\n",
        "    i=0\n",
        "    for pair in pairs:\n",
        "        i+=1\n",
        "        progressBar(value=i,endvalue=len(pairs))\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        #  input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc:\n",
        "                keep_input = False\n",
        "                break\n",
        "        #  output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"\\nTrimmed from {} pairs to {}\".format(len(pairs), len(keep_pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# # Trim voc and pairs\n",
        "pairs_final = trimRareWords(new_vocab, pairs_final)\n",
        "with open ('final_pairs_v21.pkl','wb') as f:\n",
        "    pairs_final = pickle.dump(pairs_final,f)\n",
        "    \n",
        "with open ('final_pairs_v21.pkl','rb') as f:\n",
        "    pairs_final = pickle.load(f)\n",
        "    \n",
        "pairs_final_train = pairs_final\n",
        "len(pairs_final_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " : [------------------->] 100%\n",
            "Trimmed from 145905 pairs to 114938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114938"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtP1tSpzf6FA",
        "outputId": "c8402fd4-ed93-4c49-bf4a-82cf8a0b78f4"
      },
      "source": [
        "with open ('final_pairs_v21.pkl','rb') as f:\n",
        "    pairs_final = pickle.load(f)\n",
        "    \n",
        "pairs_final_train = pairs_final\n",
        "len(pairs_final_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114938"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szo-AVa_z1tR"
      },
      "source": [
        "# Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf7PJEmwUywI"
      },
      "source": [
        "Our models will eventually expect numerical tensors as inputs, despite the fact that we put a lot of effort into preparing and massaging our data into a nice vocabulary object and list of sentence pairs. The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P8WElcGICYy",
        "outputId": "35d6ca03-f221-412b-a721-04c44f666e2b"
      },
      "source": [
        "test = False\n",
        "GRU_units = 50\n",
        "batch_size = 32\n",
        "emb_dim = 50\n",
        "init_lr = 0.001\n",
        "\n",
        "#Create an instance of the tokenizer object:\n",
        "tokenizer = Tokenizer(filters = [])\n",
        "tokenizer.fit_on_texts(new_vocab)\n",
        "\n",
        "ixtoword = {} # index to word dic\n",
        "wordtoix = tokenizer.word_index # word to index dic\n",
        "pad_token = 'pad0'\n",
        "ixtoword[0] = pad_token # no word in vocab has index 0,  padding is indicated with 0\n",
        "\n",
        "for w in tokenizer.word_index:\n",
        "    ixtoword[tokenizer.word_index[w]] = w\n",
        "\n",
        "# Making the embedding mtrix\n",
        "def make_embedding_layer(embedding_dim=50, glove=True):\n",
        "    if glove == False:\n",
        "        print('Just a zero matrix loaded')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
        "    else:\n",
        "        print('Loading glove...')\n",
        "        embeddings_index = {} \n",
        "        f = open(os.path.join('/content/glove.6B.50d.txt'), encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        print(\"GloVe \",embedding_dim, ' loded!')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
        "        for word, i in wordtoix.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words that are not found in the embedding index will be all zeros\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \n",
        "                                                                                           # do not train the embedding layer\n",
        "                                                                                           # we use 0 as padding so => mask_zero=True\n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "    \n",
        "    return embedding_layer\n",
        "\n",
        "embeddings = make_embedding_layer(embedding_dim=50, glove=not test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading glove...\n",
            "GloVe  50  loded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIZCs-L8-XWh"
      },
      "source": [
        "# Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEcMRrQ3VVwA"
      },
      "source": [
        "A sequence-to-sequence (seq2seq) model is at the core of our model. The purpose of a seq2seq model is to use a fixed-sized sequence as an input and generate a variable-length sequence as an output.\n",
        "\n",
        "[Sutskever et al.](https://arxiv.org/abs/1409.3215) found that we can do this task by combining two different recurrent neural nets. One  RNN serves as an encoder, converting a variable-length input sequence to a fixed-length context vector. This context vector (the RNN's final hidden layer) contain semantic knowledge about the query sentence that the system receives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhGdfvA0-jcE"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fHwj0C-idB"
      },
      "source": [
        "The encoder RNN iterates through the input sentence one token at a time, producing an \"output\" vector and a \"hidden state\" vector at each time step.  The output vector is recorded while the hidden state vector is transferred to the next time step. The encoder converts the context it observed at each point in the sequence into a set of points in a high-dimensional space, which the decoder can use to produce a meaningful output for the task at hand.\n",
        "\n",
        "A multi-layered Gated Recurrent Unit, created by [Cho et al.](https://arxiv.org/pdf/1406.1078v3.pdf), is at the centre of our encoder. We'll use a bidirectional version of the GRU, which effectively means there are two separate RNNs: one fed the input sequence in regular sequential order and the other fed the input sequence in reverse order. At each time point, the outputs of each network are added together.\n",
        "\n",
        "***For the first task you need to define a bidirectional GRU and pass the embedding into the GRU. ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gJCh5vafUfi"
      },
      "source": [
        "https://github.com/google/seq2seq/blob/master/seq2seq/models/basic_seq2seq.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGy-CuL-IXKr"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.enc_units = enc_units\n",
        "\n",
        "        #  pass the embedding into a bidirectional version of the GRU\n",
        "        #self.embedding_dim = embedding_dim\n",
        "        #self.embeddings = tf.keras.layers.Embedding(vocab_size+1,embedding_dim)\n",
        "        self.embeddings = embeddings\n",
        "        #self.GRU = tf.keras.layers.GRU(enc_units)\n",
        "        #self.GRU = GRU\n",
        "        self.Bidirectional1 = Bidirectional(GRU(self.enc_units, return_sequences=True))\n",
        "        self.Bidirectional2 = Bidirectional(GRU(self.enc_units, return_sequences=True, return_state=True))\n",
        "\n",
        "        #                                                                                        \n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
        "            \n",
        "    def bidirectional(self, bidir, layer, inp, hidden):\n",
        "        return bidir(layer(inp, initial_state = hidden))\n",
        "    \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.Bidirectional1(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state_f,state_b = self.Bidirectional2(x)\n",
        "\n",
        "        return output, state_f, state_b\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWX9HCkIIbed"
      },
      "source": [
        "encoder = Encoder(vocab_len, 50, GRU_units)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80-YvbB--qbT"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ7GWss5YqUC"
      },
      "source": [
        "The response utterance is produced token by token by the decoder RNN. It generates the next word in the sequence using the encoder's context vectors and internal hidden states. It keeps producing words until it reaches the end of the sentence, which is represented by an end_token. A common issue with a standard seq2seq decoder is that relying solely on the context vector to encode the meaning of the complete input sequence would almost certainly result in information loss. This is particularly true when dealing with long input sequences, severely restricting our decoder's capabilities.\n",
        "\n",
        "[Bahdanau et al.](https://arxiv.org/abs/1409.0473) devised an \"attention mechanism\" that allows the decoder to focus on specific parts of the input sequence rather than using the whole set context at each step to deal with information loss. Attention is determined using the encoder's outputs and the decoder's current hidden state. Since the output attention weights have the same shape as the input sequence, we may multiply them by the encoder outputs to get a weighted amount that shows which sections of the encoder output to focus on.\n",
        "\n",
        "**For the second task you need to create the decoder with attention. Call the attention layer and use GRUs for decoding.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KssO1KLzId2d"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.units = units\n",
        "        \n",
        "    def call(self, query, values):\n",
        "        \n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyo_ifo5IhLx"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.embeddings = embeddings\n",
        "        self.units = 2 * dec_units # because we use bidirectional encoder\n",
        "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
        "        # Cretae the decoder with attention\n",
        "        #self.GRU = tf.keras.layers.GRU(self.units)\n",
        "        #self.attention = tf.keras.layers.Attention()\n",
        "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True)\n",
        "        self.decoder_gru_l2 = GRU(self.units, return_state=True)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embeddings(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # concat input and context vector together\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        x = self.decoder_gru_l1(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state = self.decoder_gru_l2(x)\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeMvjzzVIjVR"
      },
      "source": [
        "decoder = Decoder(vocab_len, 50, GRU_units)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67WV-Fnb_VvJ"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJPZPKIpeGGN"
      },
      "source": [
        "We can now write functions to evaluate a string input sentence now that we've established our decoding process. The evaluate function is in charge of the low-level handling of the input sentence. The sentence is first formatted as an input batch of word indexes. To prepare the tensor for our models, we convert the words of the sentence to their corresponding indexes and transpose the dimensions. Our system's user interface is called answer. Our text is normalised in the same way that our training data is, and then fed into the evaluate function to generate a decoded output sentence and attention weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVSbB_kLIo3p"
      },
      "source": [
        "import unicodedata\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(5,5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def evaluate(sentence):\n",
        "    \n",
        "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
        "\n",
        "    sentence = unicode_to_ascii(sentence.lower())\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
        "\n",
        "    for t in range(max_len_a):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = K.get_value(attention_weights)\n",
        "        \n",
        "        predicted_id =  K.get_value(tf.argmax(predictions[0]))       \n",
        "\n",
        "        if ixtoword[predicted_id] == end_token:\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        result += ixtoword[predicted_id] + ' '\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "def answer(sentence, training=False):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    \n",
        "    if training:\n",
        "        return result\n",
        "    \n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))\n",
        "    attention_plot = attention_plot[1:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' ')[:-1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl-ffZTS_dtP"
      },
      "source": [
        "# Greedy decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xacfBuH7dxUw"
      },
      "source": [
        "Greedy decoding is a decoding method in which we simply choose the highest softmax value word from decoder output for each time stage. On a single time-step stage, this decoding method is optimal. It is common in neural machine translation systems to use a beam-search to sample the probabilities for the words in the sequence output by the model.\n",
        "\n",
        "The wider the beam width, the more exhaustive the search, and, it is believed, the better the results.\n",
        "\n",
        "The results showed that a modest beam-width of 3-5 performed the best, which could be improved only very slightly through the use of length penalties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa46aAvvIvsx"
      },
      "source": [
        "def beam_search(sentence, k=5, maxsample=max_len_a, use_unk=False, oov=None, eos=end_token):\n",
        "\n",
        "    \n",
        "    dead_k = 0 # samples that reached eos\n",
        "    dead_samples = []\n",
        "    dead_scores = []\n",
        "    live_k = 1 # samples that did not yet reached eos\n",
        "    live_samples = [[wordtoix[start_token]]]\n",
        "    live_scores = [0]\n",
        "\n",
        "    sentence = unicode_to_ascii(sentence.lower())\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 0)\n",
        "        \n",
        "    while live_k and dead_k < k:\n",
        "        # for every possible live sample calc prob for every possible label \n",
        "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
        "        probs = K.get_value(predictions[0])\n",
        "        # total score for every sample is sum of -log of word prb\n",
        "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
        "        if not use_unk and oov is not None:\n",
        "            cand_scores[:,oov] = 1e20\n",
        "        cand_flat = cand_scores.flatten()\n",
        "\n",
        "        # find the best (lowest) scores we have from all possible samples and new words\n",
        "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
        "        live_scores = cand_flat[ranks_flat]\n",
        "\n",
        "        # append the new words to their appropriate live sample\n",
        "        voc_size = vocab_len\n",
        "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
        "\n",
        "        # live samples that should be dead are...\n",
        "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
        "        \n",
        "        # add zombies to the dead\n",
        "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
        "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
        "        dead_k = len(dead_samples)\n",
        "        # remove zombies from the living \n",
        "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
        "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
        "        live_k = len(live_samples)\n",
        "\n",
        "    final_samples = dead_samples + live_samples\n",
        "    final_scores = dead_scores + live_scores   \n",
        "    \n",
        "    # cutting the strong where end_token is encounterd\n",
        "    for i in range(len(final_scores)):\n",
        "        final_scores[i] /= len(final_samples[i]) # normalizing the scores\n",
        "    \n",
        "    final_result =[]\n",
        "    \n",
        "    for i in range(len(final_scores)):\n",
        "        final_result.append((final_scores[i],final_samples[i]))\n",
        "    \n",
        "    final_list_ix = max(final_result)[1]\n",
        "    final_list_word = [ixtoword[f] for f in final_list_ix]\n",
        "    final_sentence = ' '.join(final_list_word[1:])\n",
        "    end_ix = final_sentence.find(end_token)\n",
        "    return final_sentence[:end_ix]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDhPN8kj-4Da"
      },
      "source": [
        "# Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYjEUAtO--f_"
      },
      "source": [
        "# Masked loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOSiJijaOwW"
      },
      "source": [
        "We can't simply consider all elements of the tensor when evaluating loss because we're dealing with batches of padded sequences. Based on our decoder's output tensor, the target tensor, and a binary mask tensor describing the padding of the target tensor, we define a function to measure our loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN3CisgKIy7W"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832td4vpI1Ok"
      },
      "source": [
        "# checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(str(emb_dim)+\"-ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbaDnfr3_Elp"
      },
      "source": [
        "# Single training iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfRcNM1TbJcs"
      },
      "source": [
        "The algorithm for a single training iteration is contained in the train_step function (a single batch of inputs). To help with convergence, we'll use teacher forcing. This means that we use the current target word as the decoder's next input rather than the decoder's current guess in some probabilities. This technique serves as decoder training wheels, allowing for more effective training. However, since the decoder may not have had enough time to truly craft its own output sequences during training, teacher forcing can lead to model instability during inference. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yeWUoboI8JK"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
        "        \n",
        "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1) # dec_input initially == start_token\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            \n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions) # each time just use one timestep output\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1) # expected output at this time becomes input for next timestep\n",
        "            \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH-SlVY_JA3n"
      },
      "source": [
        "history={'loss':[]}\n",
        "smallest_loss = np.inf\n",
        "best_ep = 1\n",
        "EPOCHS = 10 # but 150 is enough\n",
        "enc_hidden = encoder.initialize_hidden_state()\n",
        "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
        "current_ep = 1\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJApfQJ4WmT"
      },
      "source": [
        "We are creating a test_bot to monitor our training in every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsYIKbz1JC7Y"
      },
      "source": [
        "def test_bot(k = 5, beam = False):\n",
        "    print('#'*20)\n",
        "    q = 'Hello'\n",
        "    print('Greedy| Q:',q,'  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "    q = 'How are you'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'What are you doing'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "    q = 'What is your favorite restaurant'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "  \n",
        "    q = 'Do you want to go out'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('#'*20)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFs5cNX_OuX"
      },
      "source": [
        "# Training iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz9who24c69J"
      },
      "source": [
        "It's finally time to link the entire training procedure to the data. Given the passed models, optimizers, data, and so on, the function is responsible for running n iterations of training. We've already done the heavy lifting with the train_step function, so this function is self-explanatory.\n",
        "\n",
        "One thing to keep in mind is that when we save our model, the encoder and decoder parameters, the optimizer parameters, the loss, the iteration, and so on are all saved. This method of saving the model will give us the most flexibility with the checkpoint. We can use the model parameters to run inference after loading a checkpoint, or we can begin training where we left off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTthBv0b5YOY"
      },
      "source": [
        "# Trained using 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEqZLz3wJHm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16c55dea-9593-40e9-c5f0-cda7f8573c10"
      },
      "source": [
        "batch_loss = K.constant(0)\n",
        "X, y = [], []\n",
        "def plot_history():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.plot(best_ep,smallest_loss,'ro')\n",
        "    plt.plot(history['loss'],'b-')\n",
        "    plt.legend(['best','loss'])\n",
        "    plt.show()\n",
        "\n",
        "for ep in range(current_ep,EPOCHS):\n",
        "    current_ep = ep    \n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    btch = 1\n",
        "\n",
        "    for p in pairs_final_train:     \n",
        "        \n",
        "        question = p[0]\n",
        "        label = p[1]\n",
        "        # find the index of each word of the caption in vocabulary\n",
        "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
        "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
        "        # encoder input and decoder input and label\n",
        "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
        "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
        "        \n",
        "        X.append(enc_in_seq)\n",
        "        y.append(dec_out_seq)\n",
        "\n",
        "        if len(X) == batch_size :\n",
        "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "            X , y = [], []\n",
        "            btch += 1\n",
        "            if btch % (steps_per_epoch//6) == 0:\n",
        "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
        "\n",
        "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
        "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
        "    history['loss'].append(epoch_loss)\n",
        "    \n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    test_bot(k=5)\n",
        "\n",
        "    if epoch_loss < smallest_loss:\n",
        "        smallest_loss = epoch_loss\n",
        "        best_ep = ep \n",
        "        print('check point saved!')\n",
        "    \n",
        "    if ep % 5 == 0:\n",
        "        plot_history()\n",
        "        \n",
        "    print('Best epoch so far: ',best_ep)\n",
        "    print('Time  {:.3f} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    print('=' * 40)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 598 Loss: 2.0916\n",
            "Epoch 1 Batch 1196 Loss: 2.3519\n",
            "Epoch 1 Batch 1794 Loss: 2.1578\n",
            "Epoch 1 Batch 2392 Loss: 2.1203\n",
            "Epoch 1 Batch 2990 Loss: 1.9152\n",
            "Epoch 1 Batch 3588 Loss: 2.2776\n",
            "\n",
            "*** Epoch 1 Loss 2.0999 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: i am not \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  1\n",
            "Time  706.366 sec\n",
            "\n",
            "========================================\n",
            "Epoch 2 Batch 598 Loss: 1.7530\n",
            "Epoch 2 Batch 1196 Loss: 2.3766\n",
            "Epoch 2 Batch 1794 Loss: 2.3027\n",
            "Epoch 2 Batch 2392 Loss: 2.3361\n",
            "Epoch 2 Batch 2990 Loss: 1.7525\n",
            "Epoch 2 Batch 3588 Loss: 2.1857\n",
            "\n",
            "*** Epoch 2 Loss 1.8631 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: i am sorry \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am sorry \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  2\n",
            "Time  639.472 sec\n",
            "\n",
            "========================================\n",
            "Epoch 3 Batch 598 Loss: 1.6437\n",
            "Epoch 3 Batch 1196 Loss: 2.4598\n",
            "Epoch 3 Batch 1794 Loss: 2.2274\n",
            "Epoch 3 Batch 2392 Loss: 2.2354\n",
            "Epoch 3 Batch 2990 Loss: 1.8588\n",
            "Epoch 3 Batch 3588 Loss: 2.3385\n",
            "\n",
            "*** Epoch 3 Loss 1.8060 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: yes \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not a good time \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not a good time \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not a good time \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  3\n",
            "Time  643.722 sec\n",
            "\n",
            "========================================\n",
            "Epoch 4 Batch 598 Loss: 1.4517\n",
            "Epoch 4 Batch 1196 Loss: 2.4941\n",
            "Epoch 4 Batch 1794 Loss: 2.0311\n",
            "Epoch 4 Batch 2392 Loss: 2.0677\n",
            "Epoch 4 Batch 2990 Loss: 1.8735\n",
            "Epoch 4 Batch 3588 Loss: 2.2154\n",
            "\n",
            "*** Epoch 4 Loss 1.7694 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not a good time \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not a good time \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not a good time \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  4\n",
            "Time  644.395 sec\n",
            "\n",
            "========================================\n",
            "Epoch 5 Batch 598 Loss: 1.4478\n",
            "Epoch 5 Batch 1196 Loss: 2.1463\n",
            "Epoch 5 Batch 1794 Loss: 1.9305\n",
            "Epoch 5 Batch 2392 Loss: 2.0325\n",
            "Epoch 5 Batch 2990 Loss: 1.8205\n",
            "Epoch 5 Batch 3588 Loss: 2.1751\n",
            "\n",
            "*** Epoch 5 Loss 1.7421 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best epoch so far:  5\n",
            "Time  649.406 sec\n",
            "\n",
            "========================================\n",
            "Epoch 6 Batch 598 Loss: 1.5248\n",
            "Epoch 6 Batch 1196 Loss: 1.9118\n",
            "Epoch 6 Batch 1794 Loss: 1.7980\n",
            "Epoch 6 Batch 2392 Loss: 1.8981\n",
            "Epoch 6 Batch 2990 Loss: 1.6282\n",
            "Epoch 6 Batch 3588 Loss: 1.9617\n",
            "\n",
            "*** Epoch 6 Loss 1.7203 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  6\n",
            "Time  643.267 sec\n",
            "\n",
            "========================================\n",
            "Epoch 7 Batch 598 Loss: 1.6364\n",
            "Epoch 7 Batch 1196 Loss: 2.1560\n",
            "Epoch 7 Batch 1794 Loss: 2.0750\n",
            "Epoch 7 Batch 2392 Loss: 2.1617\n",
            "Epoch 7 Batch 2990 Loss: 1.5048\n",
            "Epoch 7 Batch 3588 Loss: 1.9508\n",
            "\n",
            "*** Epoch 7 Loss 1.7035 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  7\n",
            "Time  649.643 sec\n",
            "\n",
            "========================================\n",
            "Epoch 8 Batch 598 Loss: 1.5664\n",
            "Epoch 8 Batch 1196 Loss: 2.2328\n",
            "Epoch 8 Batch 1794 Loss: 2.0710\n",
            "Epoch 8 Batch 2392 Loss: 2.0901\n",
            "Epoch 8 Batch 2990 Loss: 1.7299\n",
            "Epoch 8 Batch 3588 Loss: 2.0613\n",
            "\n",
            "*** Epoch 8 Loss 1.6891 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  8\n",
            "Time  652.200 sec\n",
            "\n",
            "========================================\n",
            "Epoch 9 Batch 598 Loss: 1.4395\n",
            "Epoch 9 Batch 1196 Loss: 2.2857\n",
            "Epoch 9 Batch 1794 Loss: 2.1051\n",
            "Epoch 9 Batch 2392 Loss: 1.9498\n",
            "Epoch 9 Batch 2990 Loss: 1.7654\n",
            "Epoch 9 Batch 3588 Loss: 2.0737\n",
            "\n",
            "*** Epoch 9 Loss 1.6765 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  9\n",
            "Time  653.012 sec\n",
            "\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK75PGx14ytY"
      },
      "source": [
        "Now we can load our best model and chat with our system. We also plot the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QxHQg1SduLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adcc026a-3f72-463b-a0e4-01b04b15e769"
      },
      "source": [
        "checkpoint.restore(str(emb_dim)+\"-ckpt-9\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fee89b21d90>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr2-oAf0fBXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27efc4a5-a703-4d10-a7ae-5ad1e58af332"
      },
      "source": [
        "test_bot()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEu-hvlUfMh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "0d339735-3395-41c7-e18d-22f96fa3bed2"
      },
      "source": [
        "q = \"How old are you\"\n",
        "answer(q, training=False)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: how old are you\n",
            "Predicted answer: i am not \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q62doKoifhcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "c0e53c4a-ad79-4958-8531-02247ff21e0f"
      },
      "source": [
        "q = \"Do you drink\"\n",
        "answer(q, training=False)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: do you drink\n",
            "Predicted answer: i am not \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71GKNnQ06GzQ"
      },
      "source": [
        "You now have a good understanding of how to build a generative conversational model. If you're interested, you can customise the chatbot's behaviour by adjusting the model and training parameters, as well as the data used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M5lZUWeUYWY"
      },
      "source": [
        "history={'loss':[]}\n",
        "smallest_loss = np.inf\n",
        "best_ep = 1\n",
        "EPOCHS = 5 # but 150 is enough\n",
        "enc_hidden = encoder.initialize_hidden_state()\n",
        "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
        "current_ep = 1\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2hYbV_e5iM2"
      },
      "source": [
        "# Trained using 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pVar5XqUTSz",
        "outputId": "40350c3f-31e8-4298-adeb-40da053c4f46"
      },
      "source": [
        "batch_loss = K.constant(0)\n",
        "X, y = [], []\n",
        "def plot_history():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.plot(best_ep,smallest_loss,'ro')\n",
        "    plt.plot(history['loss'],'b-')\n",
        "    plt.legend(['best','loss'])\n",
        "    plt.show()\n",
        "\n",
        "for ep in range(current_ep,EPOCHS):\n",
        "    current_ep = ep    \n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    btch = 1\n",
        "\n",
        "    for p in pairs_final_train:     \n",
        "        \n",
        "        question = p[0]\n",
        "        label = p[1]\n",
        "        # find the index of each word of the caption in vocabulary\n",
        "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
        "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
        "        # encoder input and decoder input and label\n",
        "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
        "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
        "        \n",
        "        X.append(enc_in_seq)\n",
        "        y.append(dec_out_seq)\n",
        "\n",
        "        if len(X) == batch_size :\n",
        "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "            X , y = [], []\n",
        "            btch += 1\n",
        "            if btch % (steps_per_epoch//6) == 0:\n",
        "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
        "\n",
        "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
        "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
        "    history['loss'].append(epoch_loss)\n",
        "    \n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    test_bot(k=5)\n",
        "\n",
        "    if epoch_loss < smallest_loss:\n",
        "        smallest_loss = epoch_loss\n",
        "        best_ep = ep \n",
        "        print('check point saved!')\n",
        "    \n",
        "    if ep % 5 == 0:\n",
        "        plot_history()\n",
        "        \n",
        "    print('Best epoch so far: ',best_ep)\n",
        "    print('Time  {:.3f} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    print('=' * 40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 598 Loss: 1.5288\n",
            "Epoch 1 Batch 1196 Loss: 1.8265\n",
            "Epoch 1 Batch 1794 Loss: 1.7716\n",
            "Epoch 1 Batch 2392 Loss: 1.8024\n",
            "Epoch 1 Batch 2990 Loss: 1.6010\n",
            "Epoch 1 Batch 3588 Loss: 1.7545\n",
            "\n",
            "*** Epoch 1 Loss 1.6651 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  1\n",
            "Time  651.765 sec\n",
            "\n",
            "========================================\n",
            "Epoch 2 Batch 598 Loss: 1.5605\n",
            "Epoch 2 Batch 1196 Loss: 2.0590\n",
            "Epoch 2 Batch 1794 Loss: 2.0696\n",
            "Epoch 2 Batch 2392 Loss: 2.1083\n",
            "Epoch 2 Batch 2990 Loss: 1.5880\n",
            "Epoch 2 Batch 3588 Loss: 1.9259\n",
            "\n",
            "*** Epoch 2 Loss 1.6558 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  2\n",
            "Time  651.605 sec\n",
            "\n",
            "========================================\n",
            "Epoch 3 Batch 598 Loss: 1.5295\n",
            "Epoch 3 Batch 1196 Loss: 2.2222\n",
            "Epoch 3 Batch 1794 Loss: 2.0409\n",
            "Epoch 3 Batch 2392 Loss: 2.0883\n",
            "Epoch 3 Batch 2990 Loss: 1.6983\n",
            "Epoch 3 Batch 3588 Loss: 2.1185\n",
            "\n",
            "*** Epoch 3 Loss 1.6471 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  3\n",
            "Time  643.522 sec\n",
            "\n",
            "========================================\n",
            "Epoch 4 Batch 598 Loss: 1.3511\n",
            "Epoch 4 Batch 1196 Loss: 2.2742\n",
            "Epoch 4 Batch 1794 Loss: 1.8917\n",
            "Epoch 4 Batch 2392 Loss: 1.9519\n",
            "Epoch 4 Batch 2990 Loss: 1.7358\n"
          ]
        }
      ]
    }
  ]
}